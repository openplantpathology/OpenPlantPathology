---
title: "How Much Do We (Plant Pathologists) Value Openness and Transparency?"
description: |
  Our most recent paper examines code and data sharing practices in plant pathology and shares some ideas for what we can do to improve.
author:
  - name: Adam Sparks
    url: https://adamhsparks.netlify.app
  - name: Emerson Del Ponte
    url: https://emersondelponte.netlify.app/
date: 2023-02-14
bibliography: bibliography.bib
output:
  distill::distill_article:
    self_contained: false
preview: 
csl: phytopathology.csl
categories: 
  - R4PlantPath
  - Reproducible Research
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

We started this initiative (Open Plant Pathology) in 2018 with the idea that we would create a community in which plant pathologists could come together and share resources and ideas and encourage a freer exchange of information, code and data.
One of the reasons for this was that a few years before that, we'd started working on analysis of randomly selected plant pathology papers published from 2012 until 2021 with Kaique Alves, Zachary Foster and Nik Grünwald, which was published in Phytopathology® in January [@Sparks2023].
What we were finding as we looked at papers across 21 journals that were dedicated to plant pathology research or published specialised sections or articles in the field of plant pathology was not surprising, but still disappointing.
As a discipline, we simply do not make much, if any, effort to help ensure that others can easily reproduce our work after it is published [@Sparks2023].

We found that most articles were not reproducible according to our scoring system and failed to take advantage of open science and reproduciblity methods that would benefit both the authors and the readers.
To wit, the vast majority of articles we looked at made no attempt to share code or data, scoring "0" in our system (Figure \@ref(fig:articleScores)).

```{r articleScores, echo=TRUE, message=FALSE, warning=FALSE, fig.cap="Aggregated article scores for each of the two categories evaluated, (A) displays 'Code availability', where '0' was 'Not available or not mentioned in the publication'; '1' was 'Available upon request to the author; '2' was 'Online, but inconvenient or non-permanent (e.g., login needed, paywall, FTP server, personal lab website that may disappear, or may have already disappeared)'; and '3' was 'Freely available online to anonymous users for foreseeable future (e.g., archived using Zenodo, dataverse or university library or some other proper archiving system)'; 'NA' indicates that no code was created to conduct the work that was detectable. (B) shows 'Data availability', where '0' was 'Not available or not mentioned in the publication'; '1' was 'Available upon request to the author; '2' was 'Online, but inconvenient or non-permanent (e.g., login needed, paywall, FTP server, personal lab website that may disappear, or may have already disappeared)'; and '3' was 'Freely available online to anonymous users for foreseeable future (e.g., archived using Zenodo, dataverse or university library or some other proper archiving system)'; 'NA' indicates that no data were generated, e.g., a methods paper. Figure reproduced from [@Sparks2023] under a Creative Commons Licence using code found in [@Sparks2023a]."}

library(Reproducibility.in.Plant.Pathology)
library(ggplot2)
library(patchwork)

rrpp <- import_notes()

a <- ggplot(rrpp, aes(x = as.factor(comp_mthds_avail))) +
  geom_bar(fill = "black") +
  ylab("Count") +
  xlab("Article Score") +
  ggtitle("Code")

b <- ggplot(rrpp, aes(x = as.factor(data_avail))) +
  geom_bar(fill = "black") +
  ylab("Count") +
  xlab("Article Score") +
  ggtitle("Data")

p <- a + b
p <- p +
  plot_annotation(tag_levels = "A") &
  theme_light()

p
```

That's a pretty shocking, but unsurprising figure.

We get it.
It's just one (or more) things that you have to do when you're prepping that paper for submission.
We mean, why bother ensuring that your code and data are available.
The paper describes everything and if anyone has any questions they can just contact you, right?

Except it isn't that easy.
At least not for the readers.
A 2018 study by -@Stodden2018 found that from 

> "a random sample of 204 scientific papers published in the journal Science after the implementation of their policy in February 2011. We found that we were able to obtain artifacts from 44% of our sample and were able to reproduce the findings for 26%. We find this policy—author remission of data and code postpublication upon request—an improvement over no policy, but currently insufficient for reproducibility."

The whole article is available from [PNAS](https://www.pnas.org/doi/10.1073/pnas.1708290115) and it's well worth a read if you're at all interested, which we assume that you are if you're reading this blog post.
But going farther, @Tedersoo2021 published "_Data sharing practices and data availability upon request differ across scientific disciplines_" saying "_We observed that statements of data availability upon (reasonable) request are inefficient and should not be allowed by journals._"
We, personally, know this too well having tried and failed to get additional data missing from papers to reproduce work or models that we were interested in and authors didn't respond or if they did, were dismissive and didn't or weren't able to provide what we was looking for.

In fact, it seems that it's not that we don't want to or intend to or at least "[we say that we want to but then we don't](https://www.nature.com/articles/d41586-022-01692-1)" [@Watson2022].

But looking at Figure \@ref(fig:articleScores), it looks like we don't even mention the data or code being available (a score of "1") for the most part either in plant pathology.

So what is it then?
Just no time?
Lack of know-how?
We just don't care?
Or maybe, we haven't been provided with enough training to use the tools and enough information to realise how beneficial it really is.
There are some examples in the community though, [Open Wheat Blast](http://openwheatblast.net/) is a great example of what can be achieved when scientists collaborate openly.

Now that we've quantified the problem, we would like to see more of these and we're here to help.
Feel free to contact [any of us](https://openplantpathology.org/team.html) directly or through [Mastodon](https://scicomm.xyz/@openplantpathology), [Twitter](https://www.twitter.com/openplantpathology), [Slack](https://communityinviter.com/apps/openplantpathology/open-plant-pathology) (an open invite) or [GitHub](https://github.com/openplantpathology?type=source), we're here to help, after all, it's [our mission](https://openplantpathology.org/about.html).

## Colophon

This post was constructed using R Version ```r paste(R.Version()[c("major", "minor")], collapse = ".")``` [@R2022].

```{r reproducibility, echo=FALSE}
sessioninfo::session_info()
```
